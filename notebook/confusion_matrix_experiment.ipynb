{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15378b1d",
   "metadata": {},
   "source": [
    "# ThinkBrake: Confusion Matrix Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e081b08",
   "metadata": {},
   "source": [
    "## Import and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (14, 7),\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 100,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "    }\n",
    ")\n",
    "\n",
    "model_colors = sns.color_palette(\"dark\", 8)\n",
    "benchmark_colors = sns.color_palette(\"dark\", 10)\n",
    "method_palette = {\n",
    "    \"Baseline\": \"#2C3E50\",\n",
    "    \"Thinkless\": \"#E74C3C\",\n",
    "    \"ThinkBrake\": \"#3498DB\",\n",
    "}\n",
    "threshold_palette = sns.color_palette(\"Blues\", 6)[1:]\n",
    "bar_colors = [\n",
    "    \"#1A5276\",\n",
    "    \"#2980B9\",\n",
    "    \"#5499C7\",\n",
    "    \"#7FB3D5\",\n",
    "    \"#A9CCE3\",\n",
    "    \"#F5B7B1\",\n",
    "    \"#F1948A\",\n",
    "    \"#EC7063\",\n",
    "    \"#E74C3C\",\n",
    "    \"#B03A2E\",\n",
    "]\n",
    "line_colors = [\n",
    "    \"#1A5276\",\n",
    "    \"#B03A2E\",\n",
    "    \"#1E8449\",\n",
    "    \"#7D3C98\",\n",
    "    \"#B9770E\",\n",
    "    \"#117A65\",\n",
    "    \"#6C3483\",\n",
    "    \"#1F618D\",\n",
    "]\n",
    "\n",
    "markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"p\", \"h\", \"*\"]\n",
    "\n",
    "model_name_map = {}\n",
    "short_names = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4592aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_subplot_layout(n_items, max_cols=4):\n",
    "    if n_items == 0:\n",
    "        return 1, 1, (6, 4)\n",
    "\n",
    "    cols = min(n_items, max_cols)\n",
    "    rows = math.ceil(n_items / cols)\n",
    "    fig_width = 5.5 * cols\n",
    "    fig_height = 4.5 * rows\n",
    "    return rows, cols, (fig_width, fig_height)\n",
    "\n",
    "\n",
    "def create_dynamic_subplots(n_items, max_cols=4, sharey=False):\n",
    "    rows, cols, figsize = calc_subplot_layout(n_items, max_cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, sharey=sharey)\n",
    "\n",
    "    if n_items == 1:\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = np.array(axes).flatten()\n",
    "\n",
    "    for i in range(n_items, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def add_value_labels(ax, bars, fmt=\".1f\", rotation=0, fontsize=9, offset=0.5):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(\n",
    "                f\"{height:{fmt}}\",\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, offset),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=fontsize,\n",
    "                rotation=rotation,\n",
    "            )\n",
    "\n",
    "\n",
    "def format_model_name(name, short_names):\n",
    "    return short_names.get(name, name.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "def load_detailed_results(model_dir, benchmark, method=\"rollout\"):\n",
    "    base_path = Path(f\"../output/{model_dir}\")\n",
    "    math_benchmarks = [\"gsm8k\", \"math500\", \"aime2024\", \"aime2025\"]\n",
    "    general_benchmarks = [\"gpqa-diamond\", \"arc-challenge\"]\n",
    "\n",
    "    if benchmark in math_benchmarks:\n",
    "        category = \"math\"\n",
    "    elif benchmark in general_benchmarks:\n",
    "        category = \"general\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if method == \"rollout\":\n",
    "        file_path = base_path / category / \"rollout\" / f\"{benchmark}_result.jsonl\"\n",
    "    else:\n",
    "        file_path = (\n",
    "            base_path / category / \"thinkbrake\" / method / f\"{benchmark}_result.jsonl\"\n",
    "        )\n",
    "\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598edc04",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2647873",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_MODELS = [\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\",\n",
    "    \"Qwen_Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\",\n",
    "    \"Qwen_Qwen3-32B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"microsoft_phi-4-reasoning\",\n",
    "]\n",
    "\n",
    "INCLUDE_BENCHMARKS = [\n",
    "    \"gsm8k\",\n",
    "    \"math500\",\n",
    "    \"aime2024\",\n",
    "    \"aime2025\",\n",
    "    \"gpqa-diamond\",\n",
    "    # \"arc-challenge\",\n",
    "    # \"bfcl-v1\",\n",
    "    # \"bfcl-v2\",\n",
    "    # \"meta-tool\",\n",
    "]\n",
    "\n",
    "INCLUDE_THRESHOLDS = [\n",
    "    \"0.1\",\n",
    "    \"0.25\",\n",
    "    \"1.0\",\n",
    "    \"2.5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b166250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "METRICS_PATH = PROJECT_ROOT / \"metrics_summary.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(METRICS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "df_metrics = df_metrics[df_metrics[\"sub_category\"].isna()].copy()\n",
    "df_metrics = df_metrics[df_metrics[\"model\"].isin(INCLUDE_MODELS)].copy()\n",
    "df_metrics = df_metrics[df_metrics[\"benchmark\"].isin(INCLUDE_BENCHMARKS)].copy()\n",
    "\n",
    "benchmarks = INCLUDE_BENCHMARKS\n",
    "thresholds = sorted(INCLUDE_THRESHOLDS, key=lambda x: float(x))\n",
    "\n",
    "\n",
    "def _model_key(model_dir: str) -> str:\n",
    "    return model_name_map.get(model_dir, model_dir)\n",
    "\n",
    "\n",
    "rollout_data = {}\n",
    "thinkbrake_data = {}\n",
    "\n",
    "df_rollout = df_metrics[\n",
    "    (df_metrics[\"method\"] == \"rollout\") & (df_metrics[\"threshold\"].isna())\n",
    "]\n",
    "for _, r in df_rollout.iterrows():\n",
    "    mk = _model_key(r[\"model\"])\n",
    "    rollout_data.setdefault(mk, {})\n",
    "    rollout_data[mk][r[\"benchmark\"]] = {\n",
    "        \"accuracy\": float(r.get(\"accuracy\", 0.0) or 0.0),\n",
    "        \"avg_token_length\": float(r.get(\"avg_token_length\", 0.0) or 0.0),\n",
    "    }\n",
    "\n",
    "df_tb = df_metrics[\n",
    "    (df_metrics[\"method\"] == \"thinkbrake\") & (~df_metrics[\"threshold\"].isna())\n",
    "]\n",
    "for _, r in df_tb.iterrows():\n",
    "    mk = _model_key(r[\"model\"])\n",
    "    try:\n",
    "        t = float(r[\"threshold\"])\n",
    "    except Exception:\n",
    "        continue\n",
    "    t_str = str(t)\n",
    "    if t_str not in INCLUDE_THRESHOLDS:\n",
    "        continue\n",
    "    thinkbrake_data.setdefault(mk, {})\n",
    "    thinkbrake_data[mk].setdefault(r[\"benchmark\"], {})\n",
    "    thinkbrake_data[mk][r[\"benchmark\"]][f\"threshold_{t_str}\"] = {\n",
    "        \"accuracy\": float(r.get(\"accuracy\", 0.0) or 0.0),\n",
    "        \"avg_token_length\": float(r.get(\"avg_token_length\", 0.0) or 0.0),\n",
    "    }\n",
    "\n",
    "filtered_rollout_data = {\n",
    "    k: v\n",
    "    for k, v in rollout_data.items()\n",
    "    if k in [model_name_map.get(m, m) for m in INCLUDE_MODELS]\n",
    "}\n",
    "filtered_thinkbrake_data = {\n",
    "    k: v\n",
    "    for k, v in thinkbrake_data.items()\n",
    "    if k in [model_name_map.get(m, m) for m in INCLUDE_MODELS]\n",
    "}\n",
    "baseline_data = filtered_rollout_data\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f1075",
   "metadata": {},
   "source": [
    "## Load Detailed Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78875d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_detailed_results(model_dirs, benchmarks, best_threshold=\"0.25\"):\n",
    "    all_results = {}\n",
    "\n",
    "    for model_dir in model_dirs:\n",
    "        all_results[model_dir] = {}\n",
    "        for benchmark in benchmarks:\n",
    "            rollout_results = load_detailed_results(model_dir, benchmark, \"rollout\")\n",
    "            thinkbrake_results = load_detailed_results(\n",
    "                model_dir, benchmark, f\"threshold_{best_threshold}\"\n",
    "            )\n",
    "\n",
    "            if rollout_results and thinkbrake_results:\n",
    "                all_results[model_dir][benchmark] = {\n",
    "                    \"rollout\": rollout_results,\n",
    "                    \"thinkbrake\": thinkbrake_results,\n",
    "                }\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed3b3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_THRESHOLD = 0.1\n",
    "detailed_results = load_all_detailed_results(\n",
    "    INCLUDE_MODELS, INCLUDE_BENCHMARKS, BEST_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c58bf",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53e75eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import parse, verify\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_multiple_choice_answer(response: str) -> str:\n",
    "    patterns = [\n",
    "        r'[\"\\*]*answer[\"\\*]*\\s*[:=]\\s*[\"\\']?([A-Da-d])[\"\\']?',\n",
    "        r\"(?:the\\s+)?answer\\s+is[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"final\\s+answer[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"(?:choice|option)[:\\s]*([A-Da-d])\\b\",\n",
    "        r\"\\b([A-Da-d])\\s*$\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response, re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches[-1].upper()\n",
    "\n",
    "    standalone_matches = re.findall(r\"\\b([A-Da-d])\\b\", response)\n",
    "    if standalone_matches:\n",
    "        return standalone_matches[-1].upper()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def verify_multiple_choice(ground_truth: str, predicted: str) -> bool:\n",
    "    \"\"\"Verify if the predicted multiple choice answer matches the ground truth.\"\"\"\n",
    "    if not predicted:\n",
    "        return False\n",
    "    return ground_truth.upper().strip() == predicted.upper().strip()\n",
    "\n",
    "\n",
    "def evaluate_item(item: dict, is_multiple_choice: bool) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate a single item using the same logic as evaluate.py\n",
    "    \"\"\"\n",
    "    if is_multiple_choice:\n",
    "        ground_truth = item[\"answer\"]\n",
    "        predicted = extract_multiple_choice_answer(item[\"response\"])\n",
    "        return verify_multiple_choice(ground_truth, predicted)\n",
    "    else:\n",
    "        # Math evaluation using math_verify\n",
    "        try:\n",
    "            ground_truth = parse(f\"${item['answer']}$\")\n",
    "            predicted = parse(item[\"response\"])\n",
    "            return verify(ground_truth, predicted)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(rollout_results, thinkbrake_results, benchmark):\n",
    "    \"\"\"Compute confusion matrix comparing rollout (baseline) vs thinkbrake results\"\"\"\n",
    "    rollout_dict = {r[\"id\"]: r for r in rollout_results}\n",
    "    thinkbrake_dict = {r[\"id\"]: r for r in thinkbrake_results}\n",
    "\n",
    "    # Determine if it's a multiple choice benchmark\n",
    "    general_benchmarks = [\"gpqa-diamond\", \"mmlu-redux\"]\n",
    "    is_multiple_choice = benchmark in general_benchmarks\n",
    "\n",
    "    both_correct = 0\n",
    "    only_rollout_correct = 0\n",
    "    only_thinkbrake_correct = 0\n",
    "    both_wrong = 0\n",
    "\n",
    "    # Find common IDs\n",
    "    common_ids = set(rollout_dict.keys()) & set(thinkbrake_dict.keys())\n",
    "\n",
    "    for pid in common_ids:\n",
    "        rollout_item = rollout_dict[pid]\n",
    "        tb_item = thinkbrake_dict[pid]\n",
    "\n",
    "        # Evaluate using the same method as evaluate.py\n",
    "        rollout_correct = evaluate_item(rollout_item, is_multiple_choice)\n",
    "        tb_correct = evaluate_item(tb_item, is_multiple_choice)\n",
    "\n",
    "        if rollout_correct and tb_correct:\n",
    "            both_correct += 1\n",
    "        elif rollout_correct and not tb_correct:\n",
    "            only_rollout_correct += 1\n",
    "        elif not rollout_correct and tb_correct:\n",
    "            only_thinkbrake_correct += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "\n",
    "    return (\n",
    "        both_correct,\n",
    "        only_rollout_correct,\n",
    "        only_thinkbrake_correct,\n",
    "        both_wrong,\n",
    "        len(common_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fa10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_data = {}\n",
    "\n",
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "    confusion_data[display_name] = {}\n",
    "\n",
    "    for benchmark, data in benchmarks_data.items():\n",
    "        if data[\"rollout\"] and data[\"thinkbrake\"]:\n",
    "            cm = compute_confusion_matrix(\n",
    "                data[\"rollout\"], data[\"thinkbrake\"], benchmark\n",
    "            )\n",
    "            confusion_data[display_name][benchmark] = {\n",
    "                \"both_correct\": cm[0],\n",
    "                \"only_rollout\": cm[1],\n",
    "                \"only_thinkbrake\": cm[2],\n",
    "                \"both_wrong\": cm[3],\n",
    "                \"total\": cm[4],\n",
    "            }\n",
    "\n",
    "\n",
    "n_models = len([m for m in confusion_data if confusion_data[m]])\n",
    "if n_models > 0:\n",
    "    fig, axes = create_dynamic_subplots(n_models, max_cols=3)\n",
    "\n",
    "    for idx, (model_name, bench_data) in enumerate(confusion_data.items()):\n",
    "        if not bench_data:\n",
    "            continue\n",
    "\n",
    "        ax = axes[idx]\n",
    "\n",
    "        total_both_correct = sum(d[\"both_correct\"] for d in bench_data.values())\n",
    "        total_only_rollout = sum(d[\"only_rollout\"] for d in bench_data.values())\n",
    "        total_only_tb = sum(d[\"only_thinkbrake\"] for d in bench_data.values())\n",
    "        total_both_wrong = sum(d[\"both_wrong\"] for d in bench_data.values())\n",
    "        total = sum(d[\"total\"] for d in bench_data.values())\n",
    "\n",
    "        baseline_acc = (\n",
    "            (total_both_correct + total_only_rollout) / total * 100 if total > 0 else 0\n",
    "        )\n",
    "        tb_acc = (total_both_correct + total_only_tb) / total * 100 if total > 0 else 0\n",
    "\n",
    "        cm = np.array(\n",
    "            [\n",
    "                [total_both_correct, total_only_tb],\n",
    "                [total_only_rollout, total_both_wrong],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Baseline ✓\", \"Baseline ✗\"])\n",
    "        ax.set_yticklabels([\"ThinkBrake ✓\", \"ThinkBrake ✗\"])\n",
    "        ax.set_xlabel(\"Baseline (Rollout)\", fontweight=\"bold\")\n",
    "        ax.set_ylabel(f\"ThinkBrake (t={BEST_THRESHOLD})\", fontweight=\"bold\")\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                pct = cm[i, j] / total * 100 if total > 0 else 0\n",
    "                text = ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{cm[i, j]}\\n({pct:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=11,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{model_name}\\nBaseline: {baseline_acc:.1f}% | TB: {tb_acc:.1f}%\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Confusion Matrix\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865636b",
   "metadata": {},
   "source": [
    "## Confusion Matrix by Benchmark\n",
    "\n",
    "각 벤치마크별로 Baseline과 ThinkBrake의 정답 일치/불일치 패턴을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21f131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir, benchmarks_data in detailed_results.items():\n",
    "    display_name = short_names.get(model_name_map.get(model_dir, model_dir), model_dir)\n",
    "\n",
    "    available_benchmarks = [b for b in INCLUDE_BENCHMARKS if b in benchmarks_data]\n",
    "    n_benchmarks = len(available_benchmarks)\n",
    "\n",
    "    if n_benchmarks == 0:\n",
    "        continue\n",
    "\n",
    "    fig, axes = create_dynamic_subplots(n_benchmarks, max_cols=3)\n",
    "\n",
    "    for idx, benchmark in enumerate(available_benchmarks):\n",
    "        ax = axes[idx]\n",
    "        data = benchmarks_data[benchmark]\n",
    "\n",
    "        if not data[\"rollout\"] or not data[\"thinkbrake\"]:\n",
    "            continue\n",
    "\n",
    "        cm_result = compute_confusion_matrix(\n",
    "            data[\"rollout\"], data[\"thinkbrake\"], benchmark\n",
    "        )\n",
    "        both_correct, only_rollout, only_tb, both_wrong, total = cm_result\n",
    "\n",
    "        baseline_acc = (both_correct + only_rollout) / total * 100 if total > 0 else 0\n",
    "        tb_acc = (both_correct + only_tb) / total * 100 if total > 0 else 0\n",
    "        diff = tb_acc - baseline_acc\n",
    "\n",
    "        cm = np.array(\n",
    "            [\n",
    "                [both_correct, only_tb],\n",
    "                [only_rollout, both_wrong],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "        # Add labels\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Base ✓\", \"Base ✗\"], fontsize=9)\n",
    "        ax.set_yticklabels([\"TB ✓\", \"TB ✗\"], fontsize=9)\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                pct = cm[i, j] / total * 100 if total > 0 else 0\n",
    "                ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{cm[i, j]}\\n({pct:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "        diff_color = \"green\" if diff >= 0 else (\"red\" if diff < 0 else \"black\")\n",
    "        diff_sign = \"+\" if diff == 0 else \"\"\n",
    "        ax.set_title(\n",
    "            f\"{benchmark}\\nBase: {baseline_acc:.1f}% → TB: {tb_acc:.1f}% ({diff_sign}{diff:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "            color=diff_color,\n",
    "        )\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{display_name} - Confusion Matrix by Benchmark (t={BEST_THRESHOLD})\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinkbrake_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
