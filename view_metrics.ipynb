{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Response Metrics - Paper Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.precision\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = Path(\"metrics_summary.jsonl\")\n",
    "\n",
    "metrics = []\n",
    "with open(metrics_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        metrics.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "print(f\"Total records: {len(df)}\")\n",
    "\n",
    "df = df[df[\"valid_for_metrics\"] == True].copy()\n",
    "print(f\"Valid records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAMES = {\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\": \"Qwen3-4B-Thinking\",\n",
    "    \"Qwen_Qwen3-4B\": \"Qwen3-4B\",\n",
    "    \"Qwen_Qwen3-14B\": \"Qwen3-14B\",\n",
    "    \"Qwen_Qwen3-32B\": \"Qwen3-32B\",\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\": \"DeepSeek-R1-7B\",\n",
    "    \"microsoft_phi-4-reasoning\": \"Phi-4-reasoning\",\n",
    "}\n",
    "\n",
    "MODEL_ORDER = [\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\",  # Qwen3-4B-Thinking\n",
    "    \"Qwen_Qwen3-4B\",  # Qwen3-4B\n",
    "    \"Qwen_Qwen3-14B\",  # Qwen3-14B\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\",  # DS-R1-7B\n",
    "    \"Qwen_Qwen3-32B\",  # Qwen3-32B\n",
    "    \"microsoft_phi-4-reasoning\",  # Phi-4-reasoning\n",
    "]\n",
    "\n",
    "MODEL_THRESHOLDS = {\n",
    "    \"Qwen_Qwen3-4B-Thinking-2507\": 0.25,\n",
    "    \"Qwen_Qwen3-4B\": 0.25,\n",
    "    \"Qwen_Qwen3-14B\": 0.25,\n",
    "    \"Qwen_Qwen3-32B\": 0.25,\n",
    "    \"deepseek-ai_DeepSeek-R1-Distill-Qwen-7B\": 1.0,\n",
    "    \"microsoft_phi-4-reasoning\": 0.25,\n",
    "}\n",
    "DEFAULT_THRESHOLD = 0.25\n",
    "\n",
    "METHOD_DISPLAY = {\n",
    "    \"rollout\": \"(baseline)\",\n",
    "    \"nowait\": \"+NoWait\",\n",
    "    \"thinkless\": \"+ThinkLess\",\n",
    "    \"thinkbrake-prob\": \"+THINKBRAKE (prob)\",\n",
    "    \"thinkbrake\": \"+THINKBRAKE\",\n",
    "    \"oracle\": \"+Oracle\",\n",
    "}\n",
    "\n",
    "df[\"model_display\"] = df[\"model\"].map(MODEL_DISPLAY_NAMES).fillna(df[\"model\"])\n",
    "print(f\"Unique models: {df['model_display'].unique().tolist()}\")\n",
    "print(f\"Unique methods: {df['method'].unique().tolist()}\")\n",
    "print(f\"Unique benchmarks: {df['benchmark'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: Reasoning Benchmarks (Math + General)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_BENCHMARKS = [\n",
    "    \"gsm8k\",\n",
    "    \"math500\",\n",
    "    \"aime2024\",\n",
    "    \"aime2025\",\n",
    "    \"gpqa-diamond\",\n",
    "    \"arc-challenge\",\n",
    "]\n",
    "\n",
    "df_reasoning = df[\n",
    "    (df[\"benchmark\"].isin(REASONING_BENCHMARKS)) & (df[\"sub_category\"].isna())\n",
    "].copy()\n",
    "\n",
    "print(f\"Reasoning benchmark records: {len(df_reasoning)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reasoning_table(df_data, model_order=None):\n",
    "    benchmarks = [\n",
    "        \"gsm8k\",\n",
    "        \"math500\",\n",
    "        \"aime2024\",\n",
    "        \"aime2025\",\n",
    "        \"gpqa-diamond\",\n",
    "        \"arc-challenge\",\n",
    "    ]\n",
    "\n",
    "    if model_order is None:\n",
    "        all_models = df_data[\"model\"].unique()\n",
    "    else:\n",
    "        available_models = set(df_data[\"model\"].unique())\n",
    "        all_models = [m for m in model_order if m in available_models]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for model in all_models:\n",
    "        model_display = MODEL_DISPLAY_NAMES.get(model, model)\n",
    "        model_data = df_data[df_data[\"model\"] == model]\n",
    "        model_threshold = MODEL_THRESHOLDS.get(model, DEFAULT_THRESHOLD)\n",
    "\n",
    "        methods_to_show = [\n",
    "            \"rollout\",\n",
    "            \"nowait\",\n",
    "            \"thinkless\",\n",
    "            \"thinkbrake-prob\",\n",
    "            \"thinkbrake\",\n",
    "        ]\n",
    "\n",
    "        baseline_data = {}\n",
    "        rollout = model_data[model_data[\"method\"] == \"rollout\"]\n",
    "        for bench in benchmarks:\n",
    "            bench_data = rollout[rollout[\"benchmark\"] == bench]\n",
    "            if len(bench_data) > 0:\n",
    "                baseline_data[bench] = {\n",
    "                    \"accuracy\": bench_data[\"accuracy\"].values[0],\n",
    "                    \"tokens\": bench_data[\"avg_token_length\"].values[0],\n",
    "                }\n",
    "\n",
    "        first_row_for_model = True\n",
    "        for method in methods_to_show:\n",
    "            # Determine threshold to use\n",
    "            if method == \"thinkbrake\":\n",
    "                use_threshold = model_threshold\n",
    "            elif method == \"thinkbrake-prob\":\n",
    "                use_threshold = 0.25\n",
    "            else:\n",
    "                use_threshold = None\n",
    "\n",
    "            # Filter by method and threshold\n",
    "            if method in [\"thinkbrake\", \"thinkbrake-prob\"]:\n",
    "                method_data = model_data[\n",
    "                    (model_data[\"method\"] == method)\n",
    "                    & (model_data[\"threshold\"] == use_threshold)\n",
    "                ]\n",
    "            else:\n",
    "                method_data = model_data[model_data[\"method\"] == method]\n",
    "\n",
    "            if len(method_data) == 0:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"Model\": model_display if first_row_for_model else \"\",\n",
    "                \"Method\": METHOD_DISPLAY.get(method, method),\n",
    "            }\n",
    "            first_row_for_model = False\n",
    "\n",
    "            acc_values = []\n",
    "\n",
    "            for bench in benchmarks:\n",
    "                bench_row = method_data[method_data[\"benchmark\"] == bench]\n",
    "                if len(bench_row) > 0:\n",
    "                    acc = bench_row[\"accuracy\"].values[0]\n",
    "                    tokens = bench_row[\"avg_token_length\"].values[0]\n",
    "\n",
    "                    row[f\"{bench}_acc\"] = f\"{acc:.1f}\"\n",
    "                    acc_values.append(acc)\n",
    "                    row[f\"{bench}_tok\"] = f\"{tokens:.0f}\"\n",
    "                else:\n",
    "                    row[f\"{bench}_acc\"] = \"–\"\n",
    "                    row[f\"{bench}_tok\"] = \"–\"\n",
    "\n",
    "            # Calculate average accuracy\n",
    "            if acc_values:\n",
    "                avg_acc = np.mean(acc_values)\n",
    "                row[\"avg_acc\"] = f\"{avg_acc:.1f}\"\n",
    "\n",
    "                # Calculate average token length\n",
    "                if method != \"rollout\":\n",
    "                    all_delta_tokens = []\n",
    "                    for bench in benchmarks:\n",
    "                        bench_row = method_data[method_data[\"benchmark\"] == bench]\n",
    "                        if len(bench_row) > 0 and bench in baseline_data:\n",
    "                            tokens = bench_row[\"avg_token_length\"].values[0]\n",
    "                            all_delta_tokens.append(tokens)\n",
    "                    if all_delta_tokens:\n",
    "                        row[\"avg_tok\"] = f\"{np.mean(all_delta_tokens):.0f}\"\n",
    "                    else:\n",
    "                        row[\"avg_tok\"] = \"–\"\n",
    "                else:\n",
    "                    row[\"avg_tok\"] = \"–\"\n",
    "            else:\n",
    "                row[\"avg_acc\"] = \"–\"\n",
    "                row[\"avg_tok\"] = \"–\"\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    result_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Rename columns for display\n",
    "    rename_map = {\n",
    "        \"gsm8k_acc\": \"GSM8K Acc\",\n",
    "        \"gsm8k_tok\": \"Token\",\n",
    "        \"math500_acc\": \"MATH500 Acc\",\n",
    "        \"math500_tok\": \"Token\",\n",
    "        \"aime2024_acc\": \"AIME24 Acc\",\n",
    "        \"aime2024_tok\": \"Token\",\n",
    "        \"aime2025_acc\": \"AIME25 Acc\",\n",
    "        \"aime2025_tok\": \"Token\",\n",
    "        \"gpqa-diamond_acc\": \"GPQA-D Acc\",\n",
    "        \"gpqa-diamond_tok\": \"Token\",\n",
    "        \"arc-challenge_acc\": \"ARC-C Acc\",\n",
    "        \"arc-challenge_tok\": \"Token\",\n",
    "        \"avg_acc\": \"Avg Acc\",\n",
    "        \"avg_tok\": \"Token\",\n",
    "    }\n",
    "    result_df = result_df.rename(columns=rename_map)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Create the table with specified model order\n",
    "reasoning_table = create_reasoning_table(df_reasoning, model_order=MODEL_ORDER)\n",
    "display(Markdown(\"### Table 1: Reasoning Benchmarks\"))\n",
    "display(reasoning_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table 2: Tool Benchmarks (BFCL + MetaTool)\n",
    "\n",
    "BFCL sub-categories: parallel, parallel_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tool = df[df[\"benchmark\"].isin([\"bfcl-v1\", \"bfcl-v2\", \"meta-tool\"])].copy()\n",
    "print(f\"Tool benchmark records: {len(df_tool)}\")\n",
    "print(f\"Sub-categories: {df_tool['sub_category'].dropna().unique().tolist()}\")\n",
    "print(f\"Benchmarks: {df_tool['benchmark'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_table(\n",
    "    df_data,\n",
    "    model_order=None,\n",
    "):\n",
    "    bfcl_subcats = [\"simple\", \"multiple\", \"parallel\", \"parallel_multiple\"]\n",
    "    metatool_subcats = [\"task2_subtask1\", \"task2_subtask4\"]\n",
    "    metatool_display_names = {\n",
    "        \"task2_subtask1\": \"single\",\n",
    "        \"task2_subtask4\": \"multiple\",\n",
    "    }\n",
    "\n",
    "    if model_order is None:\n",
    "        all_models = df_data[\"model\"].unique()\n",
    "    else:\n",
    "        available_models = set(df_data[\"model\"].unique())\n",
    "        all_models = [m for m in model_order if m in available_models]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for model in all_models:\n",
    "        model_display = MODEL_DISPLAY_NAMES.get(model, model)\n",
    "        model_data = df_data[df_data[\"model\"] == model]\n",
    "        model_threshold = MODEL_THRESHOLDS.get(model, DEFAULT_THRESHOLD)\n",
    "\n",
    "        methods_to_show = [\n",
    "            \"rollout\",\n",
    "            \"nowait\",\n",
    "            \"thinkless\",\n",
    "            \"thinkbrake-prob\",\n",
    "            \"thinkbrake\",\n",
    "        ]\n",
    "\n",
    "        baseline_data = {}\n",
    "        rollout = model_data[model_data[\"method\"] == \"rollout\"]\n",
    "\n",
    "        for bench in [\"bfcl-v1\", \"bfcl-v2\"]:\n",
    "            for subcat in bfcl_subcats:\n",
    "                key = f\"{bench}_{subcat}\"\n",
    "                data = rollout[\n",
    "                    (rollout[\"benchmark\"] == bench)\n",
    "                    & (rollout[\"sub_category\"] == subcat)\n",
    "                ]\n",
    "                if len(data) > 0:\n",
    "                    baseline_data[key] = {\n",
    "                        \"accuracy\": data[\"accuracy\"].values[0],\n",
    "                        \"tokens\": data[\"avg_token_length\"].values[0],\n",
    "                    }\n",
    "\n",
    "        for subcat in metatool_subcats:\n",
    "            key = f\"meta-tool_{subcat}\"\n",
    "            data = rollout[\n",
    "                (rollout[\"benchmark\"] == \"meta-tool\")\n",
    "                & (rollout[\"sub_category\"] == subcat)\n",
    "            ]\n",
    "            if len(data) > 0:\n",
    "                baseline_data[key] = {\n",
    "                    \"accuracy\": data[\"accuracy\"].values[0],\n",
    "                    \"tokens\": data[\"avg_token_length\"].values[0],\n",
    "                }\n",
    "\n",
    "        first_row_for_model = True\n",
    "        for method in methods_to_show:\n",
    "            if method == \"thinkbrake\":\n",
    "                method_data = model_data[\n",
    "                    (model_data[\"method\"] == method)\n",
    "                    & (model_data[\"threshold\"] == model_threshold)\n",
    "                ]\n",
    "            elif method == \"thinkbrake-prob\":\n",
    "                method_data = model_data[\n",
    "                    (model_data[\"method\"] == method)\n",
    "                    & (model_data[\"threshold\"] == DEFAULT_THRESHOLD)\n",
    "                ]\n",
    "            else:\n",
    "                method_data = model_data[model_data[\"method\"] == method]\n",
    "\n",
    "            if len(method_data) == 0:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"Model\": model_display if first_row_for_model else \"\",\n",
    "                \"Method\": METHOD_DISPLAY.get(method, method),\n",
    "            }\n",
    "            first_row_for_model = False\n",
    "\n",
    "            for bench in [\"bfcl-v1\", \"bfcl-v2\"]:\n",
    "                for subcat in bfcl_subcats:\n",
    "                    key = f\"{bench}_{subcat}\"\n",
    "                    col_prefix = (\n",
    "                        f\"{bench.upper().replace('-', '')}_{subcat.replace('_','-')}\"\n",
    "                    )\n",
    "\n",
    "                    data = method_data[\n",
    "                        (method_data[\"benchmark\"] == bench)\n",
    "                        & (method_data[\"sub_category\"] == subcat)\n",
    "                    ]\n",
    "\n",
    "                    if len(data) > 0:\n",
    "                        acc = data[\"accuracy\"].values[0]\n",
    "                        tokens = data[\"avg_token_length\"].values[0]\n",
    "\n",
    "                        row[f\"{col_prefix}_acc\"] = f\"{acc:.1f}\"\n",
    "                        row[f\"{col_prefix}_tok\"] = f\"{tokens:.0f}\"\n",
    "\n",
    "                    else:\n",
    "                        row[f\"{col_prefix}_acc\"] = \"–\"\n",
    "                        row[f\"{col_prefix}_tok\"] = \"–\"\n",
    "\n",
    "            # Add MetaTool sub-category columns\n",
    "            for subcat in metatool_subcats:\n",
    "                key = f\"meta-tool_{subcat}\"\n",
    "                display_name = metatool_display_names.get(subcat, subcat)\n",
    "                col_prefix = f\"MT_{display_name}\"\n",
    "\n",
    "                data = method_data[\n",
    "                    (method_data[\"benchmark\"] == \"meta-tool\")\n",
    "                    & (method_data[\"sub_category\"] == subcat)\n",
    "                ]\n",
    "\n",
    "                if len(data) > 0:\n",
    "                    acc = data[\"accuracy\"].values[0]\n",
    "                    tokens = data[\"avg_token_length\"].values[0]\n",
    "                    row[f\"{col_prefix}_acc\"] = f\"{acc:.1f}\"\n",
    "                    row[f\"{col_prefix}_tok\"] = f\"{tokens:.0f}\"\n",
    "                else:\n",
    "                    row[f\"{col_prefix}_acc\"] = \"–\"\n",
    "                    row[f\"{col_prefix}_tok\"] = \"–\"\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Create the table with specified model order\n",
    "tool_table = create_tool_table(df_tool, model_order=MODEL_ORDER)\n",
    "display(Markdown(\"### Table 2: Tool Benchmarks (BFCL + MetaTool Sub-Categories)\"))\n",
    "display(tool_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table 3: Extended Metrics (pass@5, majority@8, avg@8)\n",
    "\n",
    "AIME와 MATH500에 대한 추가 메트릭을 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_metrics_table(\n",
    "    df_data, main_model=\"Qwen_Qwen3-4B-Thinking-2507\", model_order=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a table showing pass@5, majority@8, avg@8 for AIME and MATH500.\n",
    "    Uses model-specific thresholds from MODEL_THRESHOLDS.\n",
    "    \"\"\"\n",
    "    benchmarks = [\"math500\", \"aime2024\", \"aime2025\"]\n",
    "\n",
    "    # Filter for extended metrics benchmarks (should have pass@k, majority_accuracy, avg@8)\n",
    "    df_ext = df_data[\n",
    "        (df_data[\"benchmark\"].isin(benchmarks)) & (df_data[\"sub_category\"].isna())\n",
    "    ].copy()\n",
    "\n",
    "    if model_order is None:\n",
    "        all_models = df_ext[\"model\"].unique()\n",
    "    else:\n",
    "        available_models = set(df_ext[\"model\"].unique())\n",
    "        all_models = [m for m in model_order if m in available_models]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for model in all_models:\n",
    "        model_display = MODEL_DISPLAY_NAMES.get(model, model)\n",
    "        model_data = df_ext[df_ext[\"model\"] == model]\n",
    "        threshold = MODEL_THRESHOLDS.get(model, DEFAULT_THRESHOLD)\n",
    "\n",
    "        # Determine methods to show\n",
    "        if model == main_model:\n",
    "            methods_to_show = [\"rollout\", \"thinkbrake\"]\n",
    "        else:\n",
    "            methods_to_show = [\"rollout\", \"thinkbrake\"]\n",
    "\n",
    "        first_row_for_model = True\n",
    "        for method in methods_to_show:\n",
    "            if method in [\"thinkbrake\", \"thinkbrake-prob\"]:\n",
    "                method_data = model_data[\n",
    "                    (model_data[\"method\"] == method)\n",
    "                    & (model_data[\"threshold\"] == threshold)\n",
    "                ]\n",
    "            else:\n",
    "                method_data = model_data[model_data[\"method\"] == method]\n",
    "\n",
    "            if len(method_data) == 0:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"Model\": model_display if first_row_for_model else \"\",\n",
    "                \"Method\": METHOD_DISPLAY.get(method, method),\n",
    "            }\n",
    "            first_row_for_model = False\n",
    "\n",
    "            for bench in benchmarks:\n",
    "                bench_row = method_data[method_data[\"benchmark\"] == bench]\n",
    "                if len(bench_row) > 0:\n",
    "                    # pass@k is a dict like {\"1\": 68.75, \"5\": 82.55}\n",
    "                    pass_k = bench_row[\"pass@k\"].values[0]\n",
    "                    if isinstance(pass_k, dict) and \"5\" in pass_k:\n",
    "                        row[f\"{bench}_pass5\"] = f\"{pass_k['5']:.1f}\"\n",
    "                    else:\n",
    "                        row[f\"{bench}_pass5\"] = \"–\"\n",
    "\n",
    "                    # majority_accuracy\n",
    "                    maj = bench_row.get(\"majority_accuracy\")\n",
    "                    if maj is not None and len(maj) > 0 and pd.notna(maj.values[0]):\n",
    "                        row[f\"{bench}_maj8\"] = f\"{maj.values[0]:.1f}\"\n",
    "                    else:\n",
    "                        row[f\"{bench}_maj8\"] = \"–\"\n",
    "\n",
    "                    # avg@8\n",
    "                    avg8 = bench_row.get(\"avg@8\")\n",
    "                    if avg8 is not None and len(avg8) > 0 and pd.notna(avg8.values[0]):\n",
    "                        row[f\"{bench}_avg8\"] = f\"{avg8.values[0]:.1f}\"\n",
    "                    else:\n",
    "                        row[f\"{bench}_avg8\"] = \"–\"\n",
    "                else:\n",
    "                    row[f\"{bench}_pass5\"] = \"–\"\n",
    "                    row[f\"{bench}_maj8\"] = \"–\"\n",
    "                    row[f\"{bench}_avg8\"] = \"–\"\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    result_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Rename columns\n",
    "    rename_map = {\n",
    "        \"math500_pass5\": \"MATH500 pass@5\",\n",
    "        \"math500_maj8\": \"maj@8\",\n",
    "        \"aime2024_pass5\": \"AIME24 pass@5\",\n",
    "        \"aime2024_maj8\": \"maj@8\",\n",
    "        \"aime2025_pass5\": \"AIME25 pass@5\",\n",
    "        \"aime2025_maj8\": \"maj@8\",\n",
    "    }\n",
    "    result_df = result_df.rename(columns=rename_map)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Create extended metrics table\n",
    "extended_table = create_extended_metrics_table(df, model_order=MODEL_ORDER)\n",
    "display(Markdown(\"### Table 3: Extended Metrics (pass@5, majority@8, avg@8)\"))\n",
    "display(extended_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Analysis for ThinkBrake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_CATEGORIES = {\n",
    "    \"gsm8k\": \"math\",\n",
    "    \"math500\": \"math\",\n",
    "    \"aime2024\": \"math\",\n",
    "    \"aime2025\": \"math\",\n",
    "    \"gpqa-diamond\": \"general\",\n",
    "    \"arc-challenge\": \"general\",\n",
    "    \"bfcl-v1\": \"tool\",\n",
    "    \"bfcl-v2\": \"tool\",\n",
    "    \"meta-tool\": \"tool\",\n",
    "}\n",
    "\n",
    "df_tb = df[(df[\"method\"] == \"thinkbrake\") & (df[\"sub_category\"].isna())].copy()\n",
    "df_tb[\"category\"] = df_tb[\"benchmark\"].map(PARENT_CATEGORIES)\n",
    "\n",
    "threshold_analysis = df_tb.pivot_table(\n",
    "    index=[\"model\", \"threshold\"], columns=\"category\", values=\"accuracy\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "print(\"ThinkBrake Threshold Analysis by Category (Average Accuracy)\")\n",
    "display(threshold_analysis.style.format(\"{:.2f}\").background_gradient(axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
